# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ByujiuELzpnNTmp_DQFOkNfLzIhbSP-4
"""

import pandas as pd


data = pd.read_csv('reviews.tsv', delimiter='\t')

data

#Lowercase -'Review' column
data['Review'] = data['Review'].str.lower()

data

import string

# Remove punctuation from the 'Review' column
translator = str.maketrans('', '', string.punctuation)
data['Review'] = data['Review'].apply(lambda x: x.translate(translator))

data

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

# English stop words list
stop_words = set(stopwords.words('english'))

# Remove stop words from the 'Review' column
data['Review'] = data['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

data

import nltk
from nltk.stem import PorterStemmer

ps = PorterStemmer()

# Apply stemming to the 'Review' column
data['Stemmed_Review'] = data['Review'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))

data

from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# Apply lemmatization to the 'Review' column
data['Lemmatized_Review'] = data['Review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))
data

import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv('reviews.tsv', delimiter='\t')

review_counts = data['Liked'].value_counts()

# Plotting the Bar Graph
plt.figure(figsize=(10, 6))
review_counts.plot(kind='bar', color=['red', 'green'])
plt.title('Number of Positive and Negative Reviews')
plt.xlabel('Review Type')
plt.ylabel('Number of Reviews')
plt.xticks([0, 1], ['Negative', 'Positive'], rotation=0)
plt.tight_layout()
plt.show()

import gensim
from gensim.models import Word2Vec

# Tokenize the lemmatized reviews
sentences = [review.split() for review in data['Lemmatized_Review']]

# Train the Word2Vec model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Save the model
model.save("word2vec.model")

import numpy as np

# Function to average all word vectors in a given review
def review_vector(review, model):
    words = review.split()
    word_vectors = [model.wv[word] for word in words if word in model.wv.key_to_index]
    return np.mean(word_vectors, axis=0)

# Apply the function to each review
data['Review_Vector'] = data['Lemmatized_Review'].apply(lambda x: review_vector(x, model))

data

'''#lstm without hyperparameter tuning
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam

# Splitting data into train and test sets
X = np.stack(data['Review_Vector'].values)
y = data['Liked'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshaping for LSTM
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

# Building LSTM model
model = Sequential()
model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Validation Loss: {loss:.4f}")
print(f"Validation Accuracy: {accuracy*100:.2f}%")'''

#grid search for lstm
'''from sklearn.base import BaseEstimator, ClassifierMixin
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.optimizers import Adam, RMSprop

class KerasLSTMClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, lstm_units=100, dropout_rate=0.2, optimizer='adam', epochs=20, batch_size=64):
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.optimizer = optimizer
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = self.build_model()

    def build_model(self):
        model = Sequential()
        model.add(LSTM(self.lstm_units, input_shape=(1, 100), return_sequences=True))
        model.add(Dropout(self.dropout_rate))
        model.add(LSTM(int(self.lstm_units/2)))
        model.add(Dropout(self.dropout_rate))
        model.add(Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])
        return model

    def fit(self, X, y):
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size)
        return self

    def predict(self, X):
        return (self.model.predict(X) > 0.5).astype("int32")

    def score(self, X, y):
        _, accuracy = self.model.evaluate(X, y, verbose=0)
        return accuracy'''

'''from sklearn.model_selection import GridSearchCV

# Parameters for grid search
param_grid = {
    'lstm_units': [50, 100],
    'dropout_rate': [0.1, 0.2],
    'optimizer': ['Adam', 'RMSprop'],
    'epochs': [10, 20],
    'batch_size': [32, 64]
}

grid = GridSearchCV(KerasLSTMClassifier(), param_grid, cv=3)
grid.fit(X_train, y_train)

print("Best parameters found: ", grid.best_params_)
print("Best cross-validation score: {:.2f}".format(grid.best_score_))'''

from gensim.models import Word2Vec

w2v_model = Word2Vec.load("word2vec.model")

from keras.preprocessing.sequence import pad_sequences

# Convert each review to its word vector sequence
data['Review_Sequence'] = data['Lemmatized_Review'].apply(lambda x: [w2v_model.wv[word] for word in x.split() if word in w2v_model.wv.key_to_index])


# Pad (or truncate) sequences to have the same length
max_sequence_length = 50  # Or any other value depending on your data
X = pad_sequences(data['Review_Sequence'].tolist(), maxlen=max_sequence_length, dtype='float32', padding='post')

y = data['Liked'].values  # Assuming you have a column named 'Label' containing the target values

from keras.models import Sequential
from keras.layers import GRU, Dropout, Dense
from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = Sequential()
model.add(GRU(100, input_shape=(max_sequence_length, 100), return_sequences=True))
model.add(Dropout(0.2))
model.add(GRU(50))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)