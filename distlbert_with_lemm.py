# -*- coding: utf-8 -*-
"""distlbert with lemm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19rrgam5-7ofRbcBcXc3oOlDybE8_TnYu
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Dropout
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load the dataset
data = pd.read_csv('reviews.tsv', delimiter='\t')

# Text Preprocessing Functions
def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])

    # Stemming
    ps = PorterStemmer()
    text = ' '.join([ps.stem(word) for word in text.split()])

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

# Apply preprocessing to the dataset
nltk.download('stopwords')
nltk.download('wordnet')
data['Processed_Review'] = data['Review'].apply(preprocess_text)

# Split the dataset
train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)
val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)

# Initialize the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Function to encode the reviews
def encode_reviews(tokenizer, reviews, max_length=128):
    return tokenizer.batch_encode_plus(
        reviews,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='tf'
    )

# Encode and prepare datasets
train_encodings = encode_reviews(tokenizer, train_data['Processed_Review'].tolist(), max_length=128)
val_encodings = encode_reviews(tokenizer, val_data['Processed_Review'].tolist(), max_length=128)
test_encodings = encode_reviews(tokenizer, test_data['Processed_Review'].tolist(), max_length=128)

# Convert to TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_data['Liked'].values)).shuffle(1000).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_data['Liked'].values)).batch(32)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_data['Liked'].values)).batch(32)

# Define the custom DistilBERT model with dropout
class CustomDistilBertModel(tf.keras.Model):
    def __init__(self, model_name, num_classes, dropout_rate=0.2):
        super(CustomDistilBertModel, self).__init__()
        self.distilbert = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs):
        distilbert_output = self.distilbert(inputs)
        output = self.dropout(distilbert_output.logits)
        return output

# Instantiate the custom model
model = CustomDistilBertModel('distilbert-base-uncased', num_classes=1, dropout_rate=0.2)

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metric = tf.keras.metrics.BinaryAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Callbacks for early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)

# Train the model
model.fit(train_dataset, epochs=4, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr])

# Evaluate the model
model.evaluate(test_dataset)

#THISSSS
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Dropout

# Load the dataset
data = pd.read_csv('reviews.tsv', delimiter='\t')

# Initial split: 70% Training, 30% Combined Validation/Test
train_data, val_test_data = train_test_split(data, test_size=0.3)

# Second split: Divide the 30% Combined Validation/Test into separate sets
val_data, test_data = train_test_split(val_test_data, test_size=0.5)

# Initialize the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Function to encode the reviews
def encode_reviews(tokenizer, reviews, max_length=128):
    return tokenizer.batch_encode_plus(
        reviews,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='tf'
    )

# Encode and prepare datasets
train_encodings = encode_reviews(tokenizer, train_data['Review'].tolist())
val_encodings = encode_reviews(tokenizer, val_data['Review'].tolist())
test_encodings = encode_reviews(tokenizer, test_data['Review'].tolist())

# Convert to TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_data['Liked'].values)).shuffle(1000).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_data['Liked'].values)).batch(32)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_data['Liked'].values)).batch(32)

# Define the custom DistilBERT model with dropout
class CustomDistilBertModel(tf.keras.Model):
    def __init__(self, model_name, num_classes, dropout_rate=0.2):
        super(CustomDistilBertModel, self).__init__()
        self.distilbert = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs):
        distilbert_output = self.distilbert(inputs)
        output = self.dropout(distilbert_output.logits)
        return output

# Instantiate the custom model
model = CustomDistilBertModel('distilbert-base-uncased', num_classes=1, dropout_rate=0.2)

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metric = tf.keras.metrics.BinaryAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Callbacks for early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)

# Train the model
model.fit(train_dataset,validation_data=val_dataset, epochs=4, callbacks=[early_stopping, reduce_lr])

# Evaluate the model
model.evaluate(test_dataset)
#validation_data=val_dataset,

from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score

# First, we need to get the predictions for the test set
test_data_encodings = encode_reviews(tokenizer, test_data['Review'].tolist())
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_data_encodings))).batch(32)
y_true = test_data['Liked'].values
y_pred_raw = model.predict(test_dataset)
y_pred = tf.sigmoid(y_pred_raw).numpy().flatten()
y_pred_label = y_pred > 0.5  # To Convert probabilities to binary labels

# Now calculate the metrics
roc_auc = roc_auc_score(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred_label)
f1 = f1_score(y_true, y_pred_label)
precision = precision_score(y_true, y_pred_label)
recall = recall_score(y_true, y_pred_label)
# Calculate test accuracy
test_accuracy = accuracy_score(y_true, y_pred_label)

# Print the test accuracy
print("Test Accuracy:", test_accuracy)

# Print the metrics
print("ROC-AUC Score:", roc_auc)
print("Confusion Matrix:\n", cm)
print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)


import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_true, y_pred)

plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Dropout
import matplotlib.pyplot as plt

# dataset
data = pd.read_csv('reviews.tsv', delimiter='\t')

# Initial split: 70% Training, 30% Combined Validation/Test
train_data, val_test_data = train_test_split(data, test_size=0.3)

# Second split: Divide the 30% Combined Validation/Test into separate sets
val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)

#  number of positive and negative reviews in the training set
train_counts = train_data['Liked'].value_counts()

# number of positive and negative reviews in the validation set
val_counts = val_data['Liked'].value_counts()

#  number of positive and negative reviews in the test set
test_counts = test_data['Liked'].value_counts()

# Print the counts
print("Training set counts:\n", train_counts)
print("\nValidation set counts:\n", val_counts)
print("\nTest set counts:\n", test_counts)

classes = ['Negative', 'Positive']


fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)
fig.suptitle('Class Distribution in Datasets')


axes[0].bar(classes, train_counts, color=['red', 'green'])
axes[0].set_title('Training Set')
axes[0].set_ylabel('Number of Samples')


axes[1].bar(classes, val_counts, color=['red', 'green'])
axes[1].set_title('Validation Set')


axes[2].bar(classes, test_counts, color=['red', 'green'])
axes[2].set_title('Test Set')


plt.show()

custom_review = "didnt like restaurant ambience,it was bad but food is tasty, everyone will like this place"
# Tokenize
custom_review_encoding = encode_reviews(tokenizer, [custom_review])

# Convert to TensorFlow Dataset
custom_review_dataset = tf.data.Dataset.from_tensor_slices((dict(custom_review_encoding))).batch(1)
# Make a prediction
prediction = model.predict(custom_review_dataset)


probability = tf.sigmoid(prediction).numpy()

#  sentiment based on the probability
sentiment = 'Positive' if probability[0] > 0.5 else 'Negative'

# result
print(f"Review: '{custom_review}'\nPredicted Sentiment: {sentiment} (Probability: {probability[0]})")